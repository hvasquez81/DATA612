Data 612 Research Discussion Assignment 4
================

### Assignment

Read one or more of the articles below and consider how to counter the radicalizing effects of recommender systems or ways to prevent algorithmic discrimination.

Renee Diresta, Wired.com (2018): Up Next: A Better Recommendation System

Zeynep Tufekci, The New York Times (2018): YouTube, the Great Radicalizer

Sanjay Krishnan, Jay Patel, Michael J. Franklin, Ken Goldberg (n/a): Social Influence Bias in Recommender Systems: A Methodology for Learning, Analyzing, and Mitigating Bias in Ratings

### Response

Radicalizing effects and algorithmic discriminiation are unfortunately issues that arise in recommender systems. Specifically in collaborative filtering systems in which items are recommended based off of similarity between other users, we are already beginning the discrimination there. However, that is the process for machine learning. Systems learn by trial and error, and enhance recommendations as the system learns more about a user, and attempts to match that user to another similar user. In the process, it may begin to recommend radical material, as explained in the article on The Wired.

I believe the issue is more with the content that's availible. For example, the recommender system doesn't know that a video contains radical content unless it's flagged as being radical. You can also have organizations that spoof the recommendation system by watching videos of a certain category, and then watching videos that are rather extreme like anti-Islamic content. The user then that was watching cooking videos, gets a recommendation to watch a video on Dinesh D'Souza because it matches the history that the organization was spoofing. Rather than put the blame on the recommender system for suggesting that a cook should watch videos on an contreversial right-wing commentator, we should look at ways to exclude the radical content. In certain cases, it might work where a cook might stumble across another topic based off a user they were rated very similar too.

My suggestion would be flagging radical content and accounts, sort of like YouTube can filter out low-quality accounts. In doing so, when you have an organization that is spoofing user ratings, a random user that is watching cooking videos is no recommended anything that is flagged as radical because of their similar ratings history. Instead, the system recommends one of the videos that the spoof account rated highly. Although this is not a permanent solution, it does help prevent innocent people from being recommended radical content.
